CHAPTER 4: METHODOLOGY AND SYSTEM ARCHITECTURE

4.1 OVERALL SYSTEM ARCHITECTURE

The SentinelGuard system architecture follows a modular, layered design philosophy that promotes component independence, system maintainability, and operational flexibility. This architectural approach enables individual components to be developed, tested, and deployed independently while maintaining coherent system behavior and consistent performance characteristics.

High-Level Architecture Overview:
The system comprises six primary architectural layers that work in concert to provide comprehensive real-time anomaly detection capabilities. Each layer encapsulates specific responsibilities while exposing well-defined interfaces for inter-layer communication and external integration.

Data Ingestion Layer:
This foundational layer handles continuous acquisition of streaming NYC taxi fare data from designated APIs and external sources. The layer implements robust data validation, quality checking, and error handling mechanisms to ensure reliable data flow throughout the system. Multiple ingestion channels provide redundancy and fault tolerance, while adaptive sampling rates balance processing load with detection timeliness requirements.

Preprocessing and Feature Engineering Layer:
The second layer transforms raw fare data into optimal representations suitable for deep learning model consumption. This involves temporal sequence formation, statistical feature extraction, normalization procedures, and contextual enrichment. The layer maintains sliding windows of historical data to preserve temporal context while applying sophisticated transformation algorithms that enhance model discrimination capabilities.

Deep Learning Model Layer:
The core analytical engine implements LSTM and GRU autoencoder architectures for temporal anomaly detection. This layer manages model training, inference operations, version control, and performance monitoring. Multiple model instances provide redundancy and enable A/B testing of different architectural approaches. The layer supports dynamic loading and unloading of models to accommodate varying operational requirements.

Decision Logic and Thresholding Layer:
This critical layer applies sophisticated algorithms to convert raw model outputs into reliable anomaly classifications. Dynamic thresholding mechanisms continuously adapt to changing reconstruction error distributions, while temporal smoothing and window-level confirmation logic eliminate false positive detections. The layer incorporates configurable sensitivity settings and severity scoring to provide nuanced anomaly assessment.

Alerting and Notification Layer:
The system's operational interface generates real-time alerts through multiple channels based on detected anomalies and their severity classifications. This layer implements alert routing logic, notification scheduling, and suppression mechanisms to prevent information overload. Integration with existing incident management systems enables seamless workflow automation and response coordination.

User Interface and Monitoring Layer:
The topmost layer provides comprehensive visualization dashboards, configuration management interfaces, and analytical tools for system operators and administrators. Real-time monitoring capabilities display current system status, performance metrics, and detection results, while historical analysis features enable trend identification and performance optimization.

Figure 4.1: SentinelGuard High-Level System Architecture

[Layered Architecture Diagram]

┌─────────────────────────────────────────────────────────────────────┐
│                    USER INTERFACE & MONITORING LAYER                 │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐  │
│  │ Dashboards  │ │ Config Mgmt │ │ Analytics   │ │ Reporting   │  │
│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘  │
└─────────────────────────────────────────────────────────────────────┘
                                    │
┌─────────────────────────────────────────────────────────────────────┐
│                    ALERTING & NOTIFICATION LAYER                     │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐  │
│  │ Alert Gen.  │ │ Routing     │ │ Suppression │ │ Integration │  │
│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘  │
└─────────────────────────────────────────────────────────────────────┘
                                    │
┌─────────────────────────────────────────────────────────────────────┐
│                DECISION LOGIC & THRESHOLDING LAYER                   │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐  │
│  │ Dynamic     │ │ Temporal    │ │ Severity    │ │ Configurable│  │
│  │ Thresholding│ │ Smoothing   │ │ Scoring     │ │ Sensitivity │  │
│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘  │
└─────────────────────────────────────────────────────────────────────┘
                                    │
┌─────────────────────────────────────────────────────────────────────┐
│                   DEEP LEARNING MODEL LAYER                         │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐  │
│  │ LSTM AE     │ │ GRU AE      │ │ Model Mgmt  │ │ Performance │  │
│  │ Inference   │ │ Inference   │ │ Versioning  │ │ Monitoring  │  │
│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘  │
└─────────────────────────────────────────────────────────────────────┘
                                    │
┌─────────────────────────────────────────────────────────────────────┐
│              PREPROCESSING & FEATURE ENGINEERING LAYER               │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐  │
│  │ Sequence    │ │ Feature     │ │ Normalizat. │ │ Temporal    │  │
│  │ Formation   │ │ Extraction  │ │ Procedures  │ │ Context     │  │
│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘  │
└─────────────────────────────────────────────────────────────────────┘
                                    │
┌─────────────────────────────────────────────────────────────────────┐
│                    DATA INGESTION LAYER                             │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐  │
│  │ API Clients │ │ Validation  │ │ Buffering   │ │ Redundancy  │  │
│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘  │
└─────────────────────────────────────────────────────────────────────┘

Architectural Principles:
The system design adheres to several key architectural principles that ensure robustness, maintainability, and scalability:

Modularity: Each architectural layer represents a cohesive functional module with well-defined interfaces and minimal coupling to other layers. This enables independent development, testing, and deployment of individual components.

Scalability: The architecture supports horizontal scaling through distributed processing capabilities and load balancing mechanisms. Each layer can be replicated and distributed across multiple nodes to handle increased processing demands.

Fault Tolerance: Redundancy is built into critical components through multiple data sources, backup processing paths, and automatic failover mechanisms. The system gracefully degrades performance rather than failing completely when individual components experience issues.

Extensibility: The modular design facilitates easy addition of new features, integration with external systems, and adaptation to different operational requirements. Standard interfaces and protocols enable seamless component replacement and enhancement.

Security: Security considerations are integrated throughout all architectural layers, from secure data transmission and authentication mechanisms to access control and audit logging capabilities.

Performance: The architecture optimizes for real-time processing requirements through efficient data flow, caching strategies, and resource management mechanisms that maintain sub-second response times under normal operating conditions.

Component Specifications:
Each architectural layer comprises specific components with defined responsibilities, interfaces, and performance characteristics. Detailed specifications ensure consistent implementation and predictable behavior across all system components.

Table 4.1: System Architecture Component Specifications

Layer                    Component           Responsibilities                        Interfaces          Performance Targets
Data Ingestion          API Client          Data acquisition from sources           REST/SOAP APIs      < 100ms response
Data Ingestion          Validator           Data quality and format checking        Internal APIs       < 5ms processing
Data Ingestion          Buffer Manager      Temporary data storage and queuing      Message queues      10,000 msg/sec capacity
Preprocessing           Sequencer           Temporal sequence formation             Internal APIs       5,000 seq/sec processing
Preprocessing           Feature Extractor   Statistical feature calculation         Internal APIs       < 10ms per sequence
Preprocessing           Normalizer          Data scaling and transformation         Internal APIs       < 5ms per batch
Model Processing        LSTM Engine         LSTM autoencoder inference              Internal APIs       < 100ms per sequence
Model Processing        GRU Engine          GRU autoencoder inference               Internal APIs       < 80ms per sequence
Model Processing        Model Manager       Model loading and version control       File system/APIs    10 models simultaneous
Decision Logic          Threshold Engine    Dynamic threshold calculation          Internal APIs       < 5ms per decision
Decision Logic          Smoothing Engine    Temporal smoothing algorithms          Internal APIs       < 3ms per window
Decision Logic          Classifier          Anomaly classification logic           Internal APIs       < 2ms per point
Alerting               Alert Generator     Alert creation and formatting          Internal APIs       1,000 alerts/sec capacity
Alerting               Router              Alert distribution and routing         External APIs       < 50ms delivery
Alerting               Suppressor          Alert filtering and suppression        Internal APIs       10,000 checks/sec
User Interface         Dashboard Server    Real-time visualization serving        Web protocols       100 concurrent users
User Interface         Config Manager      System configuration management        Internal APIs       < 100ms update response
User Interface         Analytics Engine    Historical data analysis               Internal APIs       < 1 second queries

4.2 DATA PIPELINE DESIGN

The data pipeline architecture ensures efficient, reliable, and consistent flow of streaming data through all processing stages while maintaining data integrity and meeting real-time performance requirements. This pipeline design addresses the unique challenges of continuous data streams including variable arrival rates, quality variations, and temporal dependencies.

Data Flow Architecture:
The pipeline implements a staged processing approach where data moves through sequential processing units, each responsible for specific transformations and quality checks. This design enables parallel processing capabilities, fault isolation, and granular monitoring of individual pipeline segments.

Ingestion Stage:
Multiple data sources feed into the ingestion stage through dedicated client components that handle protocol-specific communication, authentication, and error recovery. The ingestion layer implements adaptive polling strategies that balance data freshness requirements with source system load considerations.

Buffering and Queue Management:
Incoming data streams are temporarily stored in high-performance buffers that provide decoupling between ingestion rate variability and downstream processing capacity. Circular buffers and message queues maintain data ordering while enabling efficient memory management and overflow handling.

Quality Assurance Processing:
The pipeline incorporates comprehensive data validation checks including format verification, range validation, completeness assessment, and consistency analysis. Invalid or suspicious data points trigger appropriate handling procedures ranging from automatic correction to alert generation for manual review.

Temporal Context Management:
Sliding window mechanisms maintain historical context necessary for sequence-based processing while efficiently managing memory utilization. Window sizes and overlap parameters are configurable to balance temporal resolution with computational requirements.

Parallel Processing Support:
The pipeline architecture enables concurrent processing of independent data segments through thread pools and distributed processing capabilities. Load balancing algorithms distribute work evenly across available processing resources while maintaining data consistency and ordering requirements.

Error Handling and Recovery:
Robust error handling mechanisms include automatic retry logic, circuit breaker patterns for failing components, and graceful degradation strategies that maintain partial functionality during system issues. Comprehensive logging and monitoring enable rapid diagnosis and resolution of pipeline problems.

Figure 4.2: Data Processing Pipeline Flow Diagram

[Flow Diagram]
External Data Sources
        │
        ▼
┌─────────────────┐
│  Ingestion      │ ◄── Multiple API Clients
│  Layer          │ ◄── Authentication & Error Handling
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Buffering &    │ ◄── Circular Buffers
│  Queue Mgmt     │ ◄── Memory Management
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Quality        │ ◄── Validation Checks
│  Assurance      │ ◄── Error Detection
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Preprocessing  │ ◄── Sequence Formation
│  Pipeline       │ ◄── Feature Extraction
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Model          │ ◄── LSTM/GRU Inference
│  Processing     │ ◄── Error Calculation
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Decision       │ ◄── Thresholding
│  Logic          │ ◄── Classification
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Output         │ ◄── Alert Generation
│  Generation     │ ◄── Result Storage
└─────────────────┘
        │
        ▼
External Systems & Users

Pipeline Performance Characteristics:
The data pipeline achieves exceptional performance through optimized algorithms, efficient memory management, and parallel processing capabilities. Key performance metrics demonstrate the system's ability to handle high-volume streaming data while maintaining real-time processing requirements.

Table 4.2: Data Pipeline Performance Characteristics

Metric                              Target Value        Actual Performance    Measurement Conditions
Maximum Throughput                  500 points/sec      380 points/sec        Peak load testing
Average Processing Latency          < 300ms             145ms                 Normal operation
Buffer Capacity                     10,000 points       8,500 points          Memory constrained
Data Loss Rate                      < 0.01%             0.003%                30-day continuous operation
Quality Validation Accuracy         > 99.5%             99.7%                 Synthetic data testing
Parallel Processing Efficiency      > 80%               85%                   8-core system utilization
Memory Utilization Under Load       < 3GB               2.1GB                 Peak processing conditions
Error Recovery Time                 < 30 sec            12 sec                Component failure scenario

4.3 DEEP LEARNING MODEL ARCHITECTURE

LSTM Autoencoder Design:
The LSTM autoencoder architecture implements a symmetric encoder-decoder structure with gated recurrent units designed specifically for temporal anomaly detection in sequential data. The encoder compresses input sequences into lower-dimensional latent representations while capturing temporal dependencies through LSTM memory cells.

Encoder Architecture:
Input Layer: Accepts normalized temporal sequences of length 3 days (72 hours) with single fare amount feature
LSTM Layer 1: 128 units with dropout regularization (0.2) for robust feature extraction
LSTM Layer 2: 64 units with return_sequences=True for intermediate representation
LSTM Layer 3: 32 units for dimensionality reduction
LSTM Layer 4: 16 units forming compressed latent representation

Decoder Architecture:
LSTM Layer 1: 32 units for initial reconstruction
LSTM Layer 2: 64 units expanding representation
LSTM Layer 3: 128 units approaching original dimensionality
Output Layer: Dense layer with single neuron for fare amount reconstruction

Training Configuration:
Optimizer: Adam with learning rate 0.001 and beta values (0.9, 0.999)
Loss Function: Mean Squared Error between input sequences and reconstructions
Batch Size: 64 sequences per training batch
Epochs: 120 with early stopping patience of 15 epochs
Validation Split: 20% of training data for monitoring overfitting

GRU Autoencoder Design:
The GRU autoencoder implements a simplified recurrent architecture that combines forget and input gates while merging cell state with hidden state. This design reduces computational complexity while maintaining competitive performance for temporal anomaly detection tasks.

Encoder Architecture:
Input Layer: Same 3-day temporal sequences as LSTM variant
GRU Layer 1: 128 units with dropout regularization (0.2)
GRU Layer 2: 64 units with return_sequences=True
GRU Layer 3: 32 units for compression
GRU Layer 4: 16 units for latent representation

Decoder Architecture:
GRU Layer 1: 32 units for reconstruction initiation
GRU Layer 2: 64 units expanding representation
GRU Layer 3: 128 units approaching original dimensions
Output Layer: Dense layer for fare amount reconstruction

Training Configuration:
Optimizer: Adam with learning rate 0.001
Loss Function: Mean Squared Error
Batch Size: 64 sequences
Epochs: 100 with early stopping
Validation Split: 20% for monitoring

Figure 4.3: LSTM Autoencoder Architecture with Attention Mechanism

[Neural Network Diagram]
Input Sequence (72 time steps)
        │
        ▼
┌─────────────────┐
│  LSTM Encoder   │ ◄── 128 → 64 → 32 → 16 units
│  Layers 1-4     │ ◄── Forget/Input/Output Gates
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Latent Space   │ ◄── 16-dimensional compressed representation
│  Representation │ ◄── Temporal pattern encoding
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  LSTM Decoder   │ ◄── 16 → 32 → 64 → 128 units
│  Layers 1-4     │ ◄── Sequential reconstruction
└─────────────────┘
        │
        ▼
Output Sequence Reconstruction

Figure 4.4: GRU Autoencoder Network Structure and Gate Operations

[Neural Network Diagram]
Input Sequence (72 time steps)
        │
        ▼
┌─────────────────┐
│  GRU Encoder    │ ◄── Update/Reset Gate Operations
│  Layers 1-4     │ ◄── 128 → 64 → 32 → 16 units
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Latent Space   │ ◄── 16-dimensional representation
│  Compression    │ ◄── Combined memory mechanism
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  GRU Decoder    │ ◄── Reconstruction processing
│  Layers 1-4     │ ◄── 16 → 32 → 64 → 128 units
└─────────────────┘
        │
        ▼
Output Sequence Reconstruction

Model Performance Comparison:
Experimental evaluation demonstrates LSTM architecture achieving superior performance with F1-score of 0.93 compared to GRU's 0.88. However, GRU training completes 23% faster and requires 18% less memory, making it valuable for resource-constrained deployment scenarios.

Table 4.3: LSTM and GRU Model Hyperparameters

Parameter              LSTM Value        GRU Value         Rationale
Hidden Units          [128,64,32,16]    [128,64,32,16]    Progressive compression
Sequence Length       72 time steps     72 time steps     3-day temporal context
Learning Rate         0.001             0.001             Standard optimization rate
Batch Size            64                64                Memory-efficient training
Dropout Rate          0.2               0.2               Regularization balance
Epochs (Max)          120               100               Early stopping consideration
Validation Split      0.2               0.2               Overfitting prevention
Optimizer             Adam              Adam              Adaptive momentum
Loss Function         MSE               MSE               Reconstruction accuracy

4.4 FEATURE ENGINEERING APPROACH

Temporal Feature Extraction:
The feature engineering pipeline transforms raw fare data into rich temporal representations that enhance model discrimination capabilities while preserving interpretability. Multiple feature categories capture different aspects of temporal patterns and relationships.

Statistical Features:
Moving averages over different time windows (1-hour, 6-hour, 24-hour) provide trend information
Rolling standard deviations measure volatility and variability patterns
Percentile-based features (25th, 50th, 75th percentiles) capture distribution characteristics
Rate of change calculations identify momentum and directional shifts

Seasonal Features:
Hour-of-day encoding captures diurnal patterns in passenger demand
Day-of-week indicators differentiate weekday vs. weekend behavior
Holiday and special event flags account for anomalous demand patterns
Monthly seasonality components reflect tourism and business cycle effects

Contextual Features:
Weather condition indicators when available improve demand prediction accuracy
Event proximity measures capture impact of concerts, sports, or conferences
Traffic condition proxies inferred from fare patterns and timing
Competitor activity indicators derived from market share estimates

Normalization Strategy:
Z-score normalization applied to all continuous features preserving meaningful magnitude relationships
Min-max scaling used for bounded categorical encodings and indicator variables
Robust scaling techniques employed for outlier-resistant feature preparation
Temporal consistency maintained through rolling window normalization approaches

Figure 4.5: Feature Engineering Pipeline and Transformation Process

[Process Flow Diagram]
Raw Fare Data Stream
        │
        ▼
┌─────────────────┐
│  Preprocessing  │ ◄── Data cleaning and validation
│  Stage          │ ◄── Missing value handling
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Temporal       │ ◄── Sequence formation (72-hour windows)
│  Segmentation   │ ◄── Sliding window generation
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Feature        │ ◄── Statistical calculations
│  Extraction     │ ◄── Seasonal encoding
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Normalization  │ ◄── Z-score transformation
│  Processing     │ ◄── Scale standardization
└─────────────────┘
        │
        ▼
Model-Ready Feature Vectors

Feature Importance Analysis:
Analysis reveals moving averages and volatility measures contribute 45% of predictive power
Seasonal features account for 30% of discrimination capability
Contextual indicators provide remaining 25% enhancement
Temporal consistency features prove most valuable for anomaly detection

Table 4.4: Feature Engineering Techniques and Transformations

Feature Category        Technique              Transformation        Impact Weight
Statistical            Moving Averages        Rolling window mean   25%
Statistical            Volatility Measures    Rolling std deviation 20%
Statistical            Percentile Features    Quantile calculation  15%
Seasonal               Hour Encoding          Cyclical sin/cos      18%
Seasonal               Day-of-Week Flags      One-hot encoding      12%
Seasonal               Holiday Indicators     Binary flags          10%
Contextual             Weather Proxies        Derived indicators    8%
Contextual             Event Proximity        Distance measures     7%
Contextual             Traffic Estimates      Pattern inference     5%
Normalization          Z-Score Scaling        Standardization       Applied to all

4.5 THRESHOLDING MECHANISMS

Dynamic Thresholding Design:
The thresholding mechanism implements adaptive anomaly detection criteria that evolve with changing data distributions while maintaining consistent false positive rates. This approach addresses concept drift challenges inherent in streaming data environments.

Percentile-Based Thresholding:
Primary threshold calculation uses 85th percentile of recent reconstruction errors
Rolling window of 50 most recent error values provides stable baseline
Adaptive recalculation every 10 data points ensures responsiveness to changes
Exponential weighting scheme gives more importance to recent observations

Statistical Process Control Integration:
Control chart principles adapted for streaming anomaly detection
Upper control limits calculated as mean + 3×standard deviation of errors
Western Electric rules implemented for pattern-based anomaly detection
Cumulative sum (CUSUM) charts monitor gradual process shifts

Multi-Level Threshold Structure:
Primary Threshold: 85th percentile for initial anomaly screening
Secondary Threshold: 95th percentile for high-confidence detections
Severity Levels: Three-tier classification based on error magnitude
Temporal Persistence: Minimum consecutive violations required for confirmation

Threshold Adaptation Algorithm:
Initialization: Bootstrap period collects 200 baseline reconstruction errors
Monitoring: Continuous evaluation of error distribution characteristics
Adaptation Trigger: Significant distribution shift detection using KS-test
Update Procedure: Gradual threshold adjustment preventing abrupt changes

Figure 4.6: Dynamic Thresholding Algorithm Flowchart

[Flowchart]
Start: New Data Point Arrival
        │
        ▼
Calculate Reconstruction Error
        │
        ▼
Update Error History Buffer
        │
        ▼
Check Distribution Stability ◄── Kolmogorov-Smirnov Test
        │
        ▼
Yes ── Calculate Percentile Threshold (85th)
        │
        No ── Gradual Threshold Adjustment
        │
        ▼
Compare Error vs Threshold
        │
        ▼
Anomaly Classification Decision
        │
        ▼
Apply Temporal Smoothing Logic
        │
        ▼
Generate Alert if Confirmed Anomaly

Mathematical Formulation:
Let E = {e₁, e₂, ..., eₙ} represent recent reconstruction errors
Primary threshold: T_primary = percentile(E, 85)
Secondary threshold: T_secondary = percentile(E, 95)
Anomaly condition: e_current > T_primary AND temporal_confirmation

4.6 DECISION LOGIC FRAMEWORK

Temporal Logic Implementation:
Production-correct temporal logic prevents single-point false positives through window-level confirmation and persistence requirements. This approach aligns with domain expert expectations for meaningful anomaly identification.

Window-Level Confirmation:
Minimum 3 consecutive anomalous windows required before point classification
Sliding window analysis examines recent 5-window history for consistency
Temporal smoothing eliminates isolated noise while preserving genuine anomalies
Sequence-to-point propagation maintains temporal context during classification

Severity Scoring System:
Reconstruction Error Magnitude: Direct correlation with anomaly severity
Temporal Persistence: Longer sustained anomalies receive higher scores
Contextual Factors: Time-of-day and seasonal adjustments applied
Composite Scoring: Weighted combination of multiple severity indicators

Decision Fusion Approach:
Multiple evidence sources combined for robust classification
LSTM and GRU model outputs weighted through ensemble voting
Temporal consistency checks validate anomaly persistence
Confidence intervals provide uncertainty quantification for decisions

Alert Generation Logic:
Priority-based alert routing based on severity scores
Configurable sensitivity settings for different operational contexts
Suppression mechanisms prevent alert flooding during sustained anomalies
Historical context analysis reduces repetitive notifications

Figure 4.7: Temporal Smoothing and Window Confirmation Logic

[Logic Diagram]
Input: Raw Anomaly Classifications
        │
        ▼
┌─────────────────┐
│  Window Buffer  │ ◄── Store recent 5 classifications
│  Management     │ ◄── Sliding window approach
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Consistency    │ ◄── Check for 3+ consecutive anomalies
│  Analysis       │ ◄── Temporal pattern validation
└─────────────────┘
        │
        ▼
┌─────────────────┐
│  Smoothing      │ ◄── Eliminate isolated false positives
│  Application    │ ◄── Apply persistence requirements
└─────────────────┘
        │
        ▼
Final Anomaly Classification Output

Decision Algorithm Pseudocode:
```
FUNCTION classify_anomaly(current_error, error_history):
    // Calculate dynamic threshold
    threshold = calculate_percentile(error_history, 85)
    
    // Initial point-level classification
    point_anomaly = (current_error > threshold)
    
    // Temporal smoothing
    recent_classifications = get_recent_classifications(5)
    window_anomaly = count_true(recent_classifications) >= 3
    
    // Sequence-to-point propagation
    if window_anomaly:
        propagate_anomaly_to_sequence_points()
    
    // Final decision with confidence scoring
    confidence = calculate_confidence(current_error, threshold)
    return (window_anomaly, confidence)
```

Performance Optimization:
Decision logic optimized for sub-millisecond execution times
Caching strategies reduce repeated calculations for overlapping windows
Pre-computed thresholds minimize real-time computational overhead
Efficient data structures enable rapid history buffer management